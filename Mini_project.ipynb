{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XV9UcH5UfT3-",
        "outputId": "aab8141c-4bd1-4a9e-b8b5-74a3fba75a30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_path = '/content/drive/MyDrive/Minor_proj/captions1.txt'\n",
        "text = open(token_path, 'r', encoding = 'utf-8').read()\n",
        "print(text[:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jnz-cVbqf7p0",
        "outputId": "c4e4bc84-5fb6-4000-b180-0c7a9d4cea5a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000268201_693b08cb0e.jpg,A child in a pink dress is climbing up a set of stairs in an entry way .\n",
            "1000268201_693b08cb0e.jpg,A girl going into a wooden building .\n",
            "1000268201_693b08cb0e.jpg,A little girl climbing into a wooden playhouse .\n",
            "1000268201_693b08cb0e.jpg,A little girl climbing the stairs to her playhouse .\n",
            "1000268201_693b08cb0e.jpg,A little girl in a pink dress going into a wooden cabin .\n",
            "1001773457_577c3a7d70.jpg,A black dog and a spotted dog are fighting\n",
            "1001773457_577c3a7d70.jpg,A bl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_description(text):\n",
        "    mapping = dict()\n",
        "    for line in text.split(\"\\n\"):\n",
        "        token = line.split(\",\")\n",
        "        if len(line) < 2:\n",
        "            continue\n",
        "        img_id = token[0].split('.')[0]\n",
        "        img_des = token[1]\n",
        "        if img_id not in mapping:\n",
        "            mapping[img_id] = list()\n",
        "        mapping[img_id].append(img_des)\n",
        "    return mapping\n",
        "\n",
        "captions= load_description(text)\n",
        "print(\"Number of items: \" + str(len(captions)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OawVdfZzgl-T",
        "outputId": "4105142f-8f77-49c6-f0cd-5133df4d1d31"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of items: 12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "def clean_description(desc):\n",
        "    for key, des_list in desc.items():\n",
        "        for i in range(len(des_list)):\n",
        "            caption = des_list[i]\n",
        "            caption = [ch for ch in caption if ch not in string.punctuation]\n",
        "            caption = ''.join(caption)\n",
        "            caption = caption.split(' ')\n",
        "            caption = [word.lower() for word in caption if len(word)>1 and word.isalpha()]\n",
        "            caption = ' '.join(caption)\n",
        "            des_list[i] = caption\n",
        "\n",
        "clean_description(captions)\n",
        "captions['1000268201_693b08cb0e']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1XeiyPpbifBy",
        "outputId": "02a302b3-f639-4e90-9296-4230db8f1148"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['child in pink dress is climbing up set of stairs in an entry way',\n",
              " 'girl going into wooden building',\n",
              " 'little girl climbing into wooden playhouse',\n",
              " 'little girl climbing the stairs to her playhouse',\n",
              " 'little girl in pink dress going into wooden cabin']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
        "\n",
        "# Function to load and preprocess images\n",
        "def load_images(directory):\n",
        "    images = {}\n",
        "    for filename in os.listdir(directory):\n",
        "        img_id = filename.split('.')[0]  # Extract image id from filename\n",
        "        img = load_img(os.path.join(directory, filename), target_size=(299, 299))\n",
        "        img = img_to_array(img)\n",
        "        img = np.expand_dims(img, axis=0)\n",
        "        img = preprocess_input(img)  # Preprocess image for InceptionV3\n",
        "        images[img_id] = img\n",
        "    return images\n",
        "\n",
        "# Path to images directory\n",
        "img_dir = '/content/drive/MyDrive/Minor_proj/images'\n",
        "\n",
        "# Load images\n",
        "img_features = load_images(img_dir)\n"
      ],
      "metadata": {
        "id": "-bI9nzukhljK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Flatten captions into list\n",
        "all_captions = [caption for captions_list in captions.values() for caption in captions_list]\n",
        "\n",
        "# Create tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(all_captions)\n",
        "\n",
        "# Vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1  # Plus one for padding (index 0)\n",
        "\n",
        "# Convert captions to sequences of integers\n",
        "sequences = tokenizer.texts_to_sequences(all_captions)\n",
        "\n",
        "# Pad sequences to maximum length\n",
        "max_length = max(len(seq) for seq in sequences)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n"
      ],
      "metadata": {
        "id": "HutowRXvhpk8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, concatenate\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "\n",
        "# Load InceptionV3 as the image feature extractor\n",
        "base_model = InceptionV3(weights='imagenet')\n",
        "image_features = base_model.layers[-2].output\n",
        "\n",
        "# Define LSTM to process captions\n",
        "caption_input = Input(shape=(max_length,))\n",
        "caption_embedding = Embedding(input_dim=vocab_size, output_dim=256, mask_zero=True)(caption_input)\n",
        "caption_lstm = LSTM(256)(caption_embedding)\n",
        "\n",
        "# Combine image and caption features\n",
        "merged = concatenate([image_features, caption_lstm])\n",
        "output_layer = Dense(vocab_size, activation='softmax')(merged)\n",
        "\n",
        "# Create final model\n",
        "model = Model(inputs=[base_model.input, caption_input], outputs=output_layer)\n",
        "\n",
        "# Freeze InceptionV3 layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzH3WdeljIvo",
        "outputId": "9be50a73-6e11-4a45-d185-aa5e7bdf636f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels.h5\n",
            "96112376/96112376 [==============================] - 1s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "token_path = '/content/drive/MyDrive/Minor_proj/captions1.txt'\n",
        "\n",
        "# Open and read the file\n",
        "with open(token_path, 'r', encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        "\n",
        "print(text[:500])\n",
        "\n",
        "def load_description(text):\n",
        "    mapping = dict()\n",
        "    for line in text.split(\"\\n\"):\n",
        "        token = line.split(\",\")\n",
        "        if len(line) < 2:\n",
        "            continue\n",
        "        img_id = token[0].split('.')[0]\n",
        "        img_des = token[1]\n",
        "        if img_id not in mapping:\n",
        "            mapping[img_id] = list()\n",
        "        mapping[img_id].append(img_des)\n",
        "    return mapping\n",
        "\n",
        "captions = load_description(text)\n",
        "print(\"Number of items: \" + str(len(captions)))\n",
        "\n",
        "import string\n",
        "def clean_description(desc):\n",
        "    for key, des_list in desc.items():\n",
        "        for i in range(len(des_list)):\n",
        "            caption = des_list[i]\n",
        "            caption = [ch for ch in caption if ch not in string.punctuation]\n",
        "            caption = ''.join(caption)\n",
        "            caption = caption.split(' ')\n",
        "            caption = [word.lower() for word in caption if len(word) > 1 and word.isalpha()]\n",
        "            caption = ' '.join(caption)\n",
        "            des_list[i] = caption\n",
        "\n",
        "clean_description(captions)\n",
        "print(captions['1000268201_693b08cb0e'])\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
        "\n",
        "# Function to load and preprocess images\n",
        "def load_images(directory):\n",
        "    images = {}\n",
        "    for filename in os.listdir(directory):\n",
        "        img_id = filename.split('.')[0]  # Extract image id from filename\n",
        "        img = load_img(os.path.join(directory, filename), target_size=(299, 299))\n",
        "        img = img_to_array(img)\n",
        "        img = np.expand_dims(img, axis=0)\n",
        "        img = preprocess_input(img)  # Preprocess image for InceptionV3\n",
        "        images[img_id] = img\n",
        "    return images\n",
        "\n",
        "# Path to images directory\n",
        "img_dir = '/content/drive/MyDrive/Minor_proj/images'\n",
        "\n",
        "# Load images\n",
        "img_features = load_images(img_dir)\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Flatten captions into list\n",
        "all_captions = [caption for captions_list in captions.values() for caption in captions_list]\n",
        "\n",
        "# Create tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(all_captions)\n",
        "\n",
        "# Vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1  # Plus one for padding (index 0)\n",
        "\n",
        "# Convert captions to sequences of integers\n",
        "sequences = tokenizer.texts_to_sequences(all_captions)\n",
        "\n",
        "# Pad sequences to maximum length\n",
        "max_length = max(len(seq) for seq in sequences)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, concatenate\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "\n",
        "# Load InceptionV3 as the image feature extractor\n",
        "base_model = InceptionV3(weights='imagenet')\n",
        "image_features = base_model.layers[-2].output\n",
        "\n",
        "# Define LSTM to process captions\n",
        "caption_input = Input(shape=(max_length,))\n",
        "caption_embedding = Embedding(input_dim=vocab_size, output_dim=256, mask_zero=True)(caption_input)\n",
        "caption_lstm = LSTM(256)(caption_embedding)\n",
        "\n",
        "# Combine image and caption features\n",
        "merged = concatenate([image_features, caption_lstm])\n",
        "output_layer = Dense(vocab_size, activation='softmax')(merged)\n",
        "\n",
        "# Create final model\n",
        "model = Model(inputs=[base_model.input, caption_input], outputs=output_layer)\n",
        "\n",
        "# Freeze InceptionV3 layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "# Define the data generator function\n",
        "def data_generator(img_features, captions, tokenizer, max_length, vocab_size, batch_size):\n",
        "    keys = list(captions.keys())\n",
        "    while True:\n",
        "        for i in range(0, len(keys), batch_size):\n",
        "            X1, X2, y = [], [], []\n",
        "            for j in range(i, min(i + batch_size, len(keys))):\n",
        "                img_id = keys[j]\n",
        "                img_feature = img_features[img_id][0]  # Adjusted to access the image correctly\n",
        "                caption_list = captions[img_id]\n",
        "                for caption in caption_list:\n",
        "                    seq = tokenizer.texts_to_sequences([caption])[0]\n",
        "                    for k in range(1, len(seq)):\n",
        "                        in_seq, out_word = seq[:k], seq[k]\n",
        "                        in_seq = pad_sequences([in_seq], maxlen=max_length, padding='post')[0]\n",
        "                        out_seq = np.zeros(vocab_size)\n",
        "                        out_seq[out_word] = 1  # Corrected line\n",
        "                        X1.append(img_feature)\n",
        "                        X2.append(in_seq)\n",
        "                        y.append(out_seq)\n",
        "            yield [np.array(X1), np.array(X2)], np.array(y)\n",
        "\n",
        "# Ensure batch_size is not larger than the number of images\n",
        "batch_size = min(64, len(img_features))  # Adjust batch size accordingly\n",
        "\n",
        "# Create the generator instance\n",
        "train_data_generator = data_generator(img_features, captions, tokenizer, max_length, vocab_size, batch_size)\n",
        "\n",
        "# Define the number of epochs and steps per epoch\n",
        "epochs = 10\n",
        "steps_per_epoch = len(img_features) // batch_size\n",
        "\n",
        "# Ensure steps_per_epoch is at least 1\n",
        "steps_per_epoch = max(steps_per_epoch, 1)\n",
        "\n",
        "# Fit the model\n",
        "model.fit(train_data_generator, epochs=epochs, steps_per_epoch=steps_per_epoch, verbose=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXaYvJP9le4C",
        "outputId": "3a83f096-35df-4c2e-f6af-5010b9b6dda3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "1000268201_693b08cb0e.jpg,A child in a pink dress is climbing up a set of stairs in an entry way .\n",
            "1000268201_693b08cb0e.jpg,A girl going into a wooden building .\n",
            "1000268201_693b08cb0e.jpg,A little girl climbing into a wooden playhouse .\n",
            "1000268201_693b08cb0e.jpg,A little girl climbing the stairs to her playhouse .\n",
            "1000268201_693b08cb0e.jpg,A little girl in a pink dress going into a wooden cabin .\n",
            "1001773457_577c3a7d70.jpg,A black dog and a spotted dog are fighting\n",
            "1001773457_577c3a7d70.jpg,A bl\n",
            "Number of items: 12\n",
            "['child in pink dress is climbing up set of stairs in an entry way', 'girl going into wooden building', 'little girl climbing into wooden playhouse', 'little girl climbing the stairs to her playhouse', 'little girl in pink dress going into wooden cabin']\n",
            "Epoch 1/10\n",
            "1/1 [==============================] - 63s 63s/step - loss: 5.3203\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - 2s 2s/step - loss: 4.7174\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - 2s 2s/step - loss: 4.3846\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - 2s 2s/step - loss: 4.1748\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - 2s 2s/step - loss: 4.0047\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - 2s 2s/step - loss: 3.8541\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - 2s 2s/step - loss: 3.7146\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - 2s 2s/step - loss: 3.5875\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - 2s 2s/step - loss: 3.4823\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - 2s 2s/step - loss: 3.3994\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7c60dc4f5750>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def sample(preds, temperature=1.0):\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds + 1e-10) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "def generate_caption(model, tokenizer, image, max_length, temperature=1.0):\n",
        "    in_text = 'startseq'\n",
        "    for _ in range(max_length):\n",
        "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
        "        preds = model.predict([image, sequence], verbose=0)[0]\n",
        "        yhat = sample(preds, temperature)\n",
        "        word = tokenizer.index_word.get(yhat, None)\n",
        "        if word is None or word == 'endseq':\n",
        "            break\n",
        "        in_text += ' ' + word\n",
        "    return in_text\n",
        "\n",
        "# Load and preprocess the image (replace 'image_path' with the actual image file path)\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "image_path = '/content/drive/MyDrive/Minor_proj/images/1000268201_693b08cb0e.jpg'\n",
        "image = load_img(image_path, target_size=(299, 299))\n",
        "image = img_to_array(image)\n",
        "image = np.expand_dims(image, axis=0)\n",
        "image = preprocess_input(image)\n",
        "\n",
        "# Generate a caption for the image\n",
        "caption = generate_caption(model, tokenizer, image, max_length, temperature=1.0)\n",
        "print('Generated Caption:', caption)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6KsZMNHle9E",
        "outputId": "07f64bb3-8d3a-4792-f1c4-641290ddf203"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Caption: startseq at dress dress wooden her wooden of an dress dress stairs pink an an dress wooden set into the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3PiupG3mle_5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}